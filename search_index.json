[
["index.html", "Community contributions for EDAV Fall 2019 Chapter 1 Instructions 1.1 Background 1.2 Preparing your .Rmd file 1.3 Submission Steps 1.4 Optional tweaks 1.5 FAQ", " Community contributions for EDAV Fall 2019 2019-10-29 Chapter 1 Instructions This chapter gives you all the information you need to upload your community contribution. Please read this entire document carefully before making your submission. Of particular note is the fact that bookdown requires a different .Rmd format than you’re used to, so you must make changes to the beginning of the file as described below before submitting. 1.1 Background This web site makes use of the bookdown package to render a collection of .Rmd files into a nicely formatted online book with chapters and subchapters. Your job will be to submit a slightly modified version of your community contribution .Rmd file to the GitHub repository where the source files for this web site are stored. On the backend, the admins will divide the chapters into book sections and order them. We use Travis CI to render the book and push the rendered .html files to our gh-pages branch–you can view our builds here–and GitHub Pages to host the site. If your community contribution is in a different format, then create a short .Rmd file that explains what you did, and includes links to any relevant files, such as slides, etc. which you can post on your GitHub repo (or another online site.) 1.2 Preparing your .Rmd file You should only submit ONE Rmd file. After completing these modifications, your .Rmd should look like this sample bookdown .Rmd. Create a concise, descriptive name for your project. For instance, name it base_r_ggplot_graph or something similar if your work is about contrasting/working with base R graphics and ggplot2 graphics. Check the .Rmd filenames in the project repo to make sure your name isn’t already taken. Your project name should be words only and joined with underscores, i.e. Do not include whitespace in the name. Create a copy of your .Rmd file with the new name. Completely delete the YAML header (the section at the top of the .Rmd that includes name, title, date, output, etc.) including the --- line. Choose a short, descriptive, human readable title for your project as your title will show up in the table of contents – look at examples in the rendered book. Capitalize the first letter only (“sentence case”). On the first line of your document, enter a single hashtag, followed by a single whitespace, and then your title. It is important to follow this format so that bookdown renders your title as a header. Do not use single # headers anywhere else in the document. The second line should be blank, followed by your name(s): # Base R vs. ggplot2 Aaron Burr and Alexander Hamilton Your content starts here. If your project requires data, please use a built-in dataset or read directly from a URL, such as: df &lt;- readr::read_csv(&quot;https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv&quot;) If you absolutely must include a data file, please use a small one, as for many reasons it is desirable to keep the repository size as small as possible. If you have included a setup chunk in your Rmd file, please remember to remove the label setup in the chunk, ie., use : {r, include=False} instead of {r setup, include=False} Want to get fancy? See the optional tweaks section below. 1.3 Submission Steps To submit your work, we will be following the instructions in this tutorial, which are provided in abbreviated form below, with specific instructions on naming conventions, content information, and other important details. Fork cc19 repo (this repo) to your GitHub account. Clone/download the forked repo to your local computer. Create a new branch and name it with your project name, in our case sample_project. If you forget to do so, check this tutorial to fix. Copy your modified .Rmd file with the same name into the root directory on the branch. In our example, it is sample_project.Rmd. Do not include an .html file. (In order for the bookdown package to work, all .Rmd files will be rendered behind the scenes.) [OPTIONAL] If you have other resources (such as images) included in your project, create a folder under resources/. In our example, it is resources/sample_project/. Put the resources files there. When you are ready to submit your project, push your branch to your remote repo. Follow this tutorial to create a pull request. If you follow the steps, we will merge it to the master branch. After submitting your pull request, do not be concerned if you see an “All builds have failed” message from Travis CI. There are things that need to be done on the backend, such as adding the libraries you use to the project for the Travis CI build to pass. 1.4 Optional tweaks If you prefer for links from your chapter to open in new tabs, add {target=&quot;_blank&quot;} after the link, such as: [edav.info](edav.info){target=&quot;_blank&quot;} Note that your headers (##, ###, etc.) will be converted to numbered headings as such: ## –&gt; 3.1 ### –&gt; 3.1.1 These headings will appear as chapter subheadings and sub-subheadings in the navigation panel on the left. Think about a logical structure for users to navigate your chapter. We recommend using only ## and ### headings as subheadings such as 4.1.3.4 are generally not necessary and look messy. Unfortunately, there’s no simple way to preview your chapter before it’s actually merged into the project. (bookdown has preview_chapter() option but it only works after the entire book has been rendered at least once and that will become more and more complex and require more and more packages as the project grows.) If you really want to preview it, fork and clone this minimal bookdown repo, add your .Rmd file, click the “Build book” button on the Build tab (next to Git), and then open any of the .html files in the _book folder in a web browser to see the rendered book. (Do not click the Knit button as it will not build a bookdown book.) If you’re interested in more bookdown options, see the official reference book. Have more useful tweaks to share? Submit an issue or PR. 1.5 FAQ 1.5.1 What should I expect after creating a pull request? Within a week after you create a pull request, we will apply a label to it and assign an administrater who will review all the files you submit to see if they meet the requirements. It will take some time before we can process all the pull requests, so as long as you see your pull request has been labeled and assigned to an administrater, don’t worry. However, if the admin contacts you regarding the pull request, that usually means your files fail to meet some requirements. The admin will clearly state what is wrong, so please fix them as soon as possible. 1.5.2 What if I catch mistakes after my pull request is merged? You may submit additional pull requests to fix material on the site. If the edits are small, such as fixing typos, it is easiest to make the edits directly on GitHub, following these instructions. We will merge first pull requests before edits, so please be patient. 1.5.3 Other questions If you encounter other problems, please submit an issue and we will look into it. Thank you for your contributions! "],
["sample-project.html", "Chapter 2 Sample project", " Chapter 2 Sample project Nancy Pelosi and Donald Trump This chapter gives a sample layout of your Rmd file. Test Photo "],
["ridgeline-plots.html", "Chapter 3 Ridgeline plots 3.1 Overview 3.2 tl;dr 3.3 Simple examples 3.4 Theory 3.5 External resources", " Chapter 3 Ridgeline plots Hojin Lee and Hyuk Joon Kwon 3.1 Overview Ridgeline plot is a set of overlapped density plots, and it helps us to compare multiple distirbutions among dataset. Professor Claus O. Wilke from UT Austin, who created ggridges package, commented about ridgeline plot as below: “Ridgeline plots are partially overlapping line plots that create the impression of a mountain range. They can be quite useful for visualizing changes in distributions over time or space. These types of plots have also been called “joyplots”, in reference to the iconic cover art for Joy Division’s album Unknown Pleasures. However, given the unfortunate origin of the name Joy Division, the term “joyplot” is now discouraged.&quot; In this section, we will discuss how to create ridgeline plots using the ggplot and ggridges libraries. 3.2 tl;dr For those who do not want to go through the documents, the below is the polished version of a ridgeline plot and the codes. library(ucidata) library(ggplot2) library(ggridges) library(viridis) library(plyr) library(nycflights13) weather$month &lt;- as.factor(weather$month) ggplot(weather, aes(x = temp, y = reorder(month, desc(month)), fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines = TRUE , geom = &quot;density_ridges_gradient&quot; , alpha = 0.6 , scale = 2.3) + scale_fill_viridis(discrete = TRUE , name = &quot;Quantile&quot; , alpha = 0.3 , option = &quot;cividis&quot;) + ggtitle(&quot;What is the weather like in NYC?&quot;, subtitle = &quot;Ridgeline plot for NYC temperature by months&quot;) + xlab(&quot;Temperature (F)&quot;) + ylab(&quot;Months&quot;) + labs(caption = &quot;Source: nycflights13::weather&quot;) + theme(plot.title = element_text(face=&quot;bold&quot;)) + theme(plot.subtitle = element_text(face=&quot;bold&quot;, color=&quot;grey&quot;)) + theme(plot.caption=element_text(color=&quot;grey&quot;)) ## Picking joint bandwidth of 1.58 ## Warning: Removed 1 rows containing non-finite values (stat_density_ridges). For more information about dataset, type ?nycflights13::weather into the console. 3.3 Simple examples For one who needs friendly step by step approach, please read the below. First, we need to install ggridges and ggplot2 packages. #install.packages(&quot;ggridges&quot;) #install.packages(&quot;ggplot2&quot;) Make sure that the y variable is a categorical variable, otherwise the function will throw an error. You can use y = as.factor(data) to transfrom your y variable into a categorical variable. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = DMC, y = day)) + geom_density_ridges() ## Picking joint bandwidth of 21 If you do not want the ridgeline plot to touch each other, please use the scale variable. A scale of 1.0 will make the adjust graph to barely touch each other. If the scale is greater than 1 the graphs will overlap with each other. Otherwise, if the scale is less than 1 the graphs will not touch each other. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = DMC, y = day)) + geom_density_ridges(scale = 1.1) ## Picking joint bandwidth of 21 There is a raindrop function within ridgeline plots, which combine the rideline plots with scatter plots; the function will plot scatter plot under the rideline plot. library(ISwR) data2 &lt;- red.cell.folate ggplot(data2, aes(x = folate, y = ventilation)) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , geom=&quot;density_ridges_gradient&quot; , jittered_points = TRUE , position = &quot;raincloud&quot; , alpha = 0.6 , scale = 0.6) ## Picking joint bandwidth of 24.5 Morevoer, it is possible to divide the data into quantiles and draw lines in between. This way, it would be easier for us to observe the median value and the interquartile range. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = temp, y = day, fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines =TRUE , geom=&quot;density_ridges_gradient&quot;) + scale_fill_viridis(discrete = TRUE , name = &quot;Quantile&quot; , option = &quot;plasma&quot;) ## Picking joint bandwidth of 1.93 In below, we have merged all the functions we have introduced, and here is the result! data2 &lt;- red.cell.folate ggplot(data2, aes(x = folate, y = ventilation, fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines = TRUE , geom=&quot;density_ridges_gradient&quot; , jittered_points = TRUE , position = &quot;raincloud&quot; , alpha = 0.6 , scale = 0.6) + scale_fill_viridis(discrete=TRUE , name = &quot;Quantile&quot; , alpha = 0.3 , option = &quot;cividis&quot;) ## Picking joint bandwidth of 24.5 Here is one last cool feature of ridgeline plots where we can overlap distributions within same data group. This enables us to compare distributions not only among different data groups but also within same data groups. library(vcd) data3 &lt;- Arthritis ggplot(data3) + geom_density_ridges(aes(x = Age, y = Treatment, group = interaction(Treatment,Improved),fill = Improved), alpha = 0.7) ## Picking joint bandwidth of 5.42 3.4 Theory Ridgeline plots are an overlap of histograms over y-axis, and this allows us to visualize and compare overall shape of distribution among different groups. They work very well when the dataset has high number of groups to show. Also, since we are overlapping distributions, we can save space for graphs. In other words, if the number of groups to represent is too small, plotting ridgeline plots might not be an optimal choice for data visualization. On the other hand, the ridgeline plots work well when there are clear differences in distributions. Otherwise, because of overlaps, they would cause more confusion when deciphering the data. Couple points to think about before plotting the ridgeline plots: Ordering of groups will change overall shape of the plots. Figure out the optimal bin size &amp; bandwidth argument for the visualization. 3.5 External resources The below has more examples with the ridgeline plots: https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html https://cmdlinetips.com/2018/03/how-to-plot-ridgeline-plots-in-r/ "],
["likert.html", "Chapter 4 Likert 4.1 Overview 4.2 tl;dr 4.3 Simple examples 4.4 Stacked bar chart using ggplot 4.5 Theory 4.6 When to use 4.7 External resources", " Chapter 4 Likert Shijie He and Chutian Chen 4.1 Overview This section covers how to make stacked bar chart on likert data. Likert data is the data with likert scale. Likert scale is a several point scale which is used to allow people to express how much they agree or disagree with a particular statement. And It’s commonly used in survey and research. 4.2 tl;dr Here’s a stacked bar chart of angry levels: And here’s the code: library(HH) library(dplyr) # create data data = data.frame(&quot;Not_at_all_angry&quot;=c(0.11,0.08,0.09,0.08,0.09,0.12,0.05,0.08),&quot;Not_very_angry&quot;=c(0.75,0.75,0.74,0.70,0.78,0.68,0.86,0.71),&quot;Fairly_angry&quot;=c(0.13,0.14,0.16,0.17,0.11,0.18,0.06,0.19),&quot;Very_angry&quot;=c(0.02,0.02,0.02,0.05,0.02,0.02,0.03,0.01),&quot;Region&quot;=c(&quot;North&quot;,&quot;Midlands&quot;,&quot;East&quot;,&quot;London&quot;,&quot;South&quot;,&quot;Wales&quot;,&quot;Scotland&quot;,&quot;Northern_Ireland&quot;),&quot;England&quot; = c(&quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;Not England&quot;, &quot;Not England&quot;, &quot;Not England&quot;)) # make stacked bar chart likert(Region ~.|England, layout=c(1,2), data, positive.order = TRUE, scales=list(y=list(relation=&quot;free&quot;)), strip.left=strip.custom(bg=&quot;gray97&quot;), strip=FALSE, as.percent = &quot;noRightAxis&quot;, ReferenceZero = 2.5, main = &#39;Angry levels in different regions&#39;, ylab = &quot;Region&quot;, xlab = &quot;Percentage&quot;, sub= list(&quot;Angry Level Rating&quot;,x=unit(.6, &quot;npc&quot;))) For more info on this dataset, go to https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/v6iuiikyxq/YG-X-MarksTheSpot-BrandsAnger-280812.pdf. 4.3 Simple examples Too complicated! Let’s see some simpler examples first. For the below examples, we will use the survey of the satisfaction of Trump for male and female. data_2 &lt;- data.frame(&quot;Great&quot;=c(4,2),&quot;Good&quot;=c(14,6),&quot;Average&quot;=c(15,16),&quot;Poor&quot;=c(17,17),&quot;Terrible&quot;=c(44,48),&quot;Gender&quot;=c(&quot;Male&quot;,&quot;Female&quot;)) data_2 &lt;- data_2[,c(6,1,2,3,4,5)] data_2 ## Gender Great Good Average Poor Terrible ## 1 Male 4 14 15 17 44 ## 2 Female 2 6 16 17 48 For more info on this dataset, go to https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/psse08hgpj/YouGov%20-%20Trump%20state%20visit%20190520.pdf. 4.3.1 Stacked bar chart To create a stacked bar chart, we will simply use likert function. likert(Gender ~ ., data_2, ReferenceZero = 0, as.percent = &quot;noRightAxis&quot;, main = &quot;Satisfaction of Trump&quot;) It is easy to compare the end values using stacked bar chart, but it is hard to compare the neutral percentage. 4.3.2 Diverging stacked bar chart To create a diverging stacked bar chart, we will modify ReferenceZero to the neutral category. likert(Gender ~ ., data_2, ReferenceZero = 3, as.percent = &quot;noRightAxis&quot;, main = &quot;Satisfaction of Trump&quot;) It is easy to visualize the overall shape of likes and dislikes, and the percentage of neutrals. However, it is hard to compare the value of like and dislike categories. 4.4 Stacked bar chart using ggplot In fact, we can make likert plot using ggplot. There is a easy way through dividing the data into two parts which represent agreement and disagreement respectively. Then we can generate the two plots on same place with one of the value of plot is negative. If there is neutral option in data, we have to divide it into two parts, which will be a little more complicated. library(ggplot2) library(reshape2) library(RColorBrewer) library(dplyr) library(ggthemes) library(stringr) library(forcats) d &lt;- data_2 d[,2:6] &lt;- d[,2:6]/rowSums(d[,2:6]) mytitle &lt;- &quot;Satisfaction of Trump&quot; mylevels &lt;- c(&quot;Great&quot;, &quot;Good&quot;, &quot;Average&quot;, &quot;Poor&quot;, &quot;Terrible&quot;) # Generate mid value of neutral category numlevels &lt;- length(d[1,])-1 numcenter &lt;- ceiling(numlevels/2) + 1 d$midvalues &lt;- d[,numcenter]/2 d_2&lt;-cbind(d[,1],d[,2:ceiling(numlevels/2)], d$midvalues, d$midvalues,d[,numcenter:numlevels+1]) colnames(d_2)&lt;-c(&quot;Sex&quot;,mylevels[1:floor(numlevels/2)],&quot;Midlow&quot;, &quot;Midhigh&quot;,mylevels[numcenter:numlevels]) # Split into six categories numlevels&lt;-length(mylevels)+1 point1&lt;-2 point2&lt;-((numlevels)/2)+1 point3&lt;-point2+1 point4&lt;-numlevels+1 # Assign color to each categories numlevels&lt;-length(d[1,])-1 temp.rows&lt;-length(d_2[,1]) pal&lt;-brewer.pal((numlevels-1),&quot;RdBu&quot;) pal[ceiling(numlevels/2)]&lt;-&quot;#DFDFDF&quot; legend.pal&lt;-pal pal&lt;-c(pal[1:(ceiling(numlevels/2)-1)], pal[ceiling(numlevels/2)], pal[ceiling(numlevels/2)], pal[(ceiling(numlevels/2)+1):(numlevels-1)]) # Generate new data frame including all information d_3&lt;-melt(d_2,id=&quot;Sex&quot;) d_3$col&lt;-rep(pal,each=temp.rows) d_3$value&lt;-d_3$value*100 d_3$Sex&lt;-str_wrap(d_3$Sex, width = 40) d_3$Sex&lt;-factor(d_3$Sex, levels = d_2$Sex[order(-(d_2[,5]+d_2[,6]+d_2[,7]))]) highs&lt;-na.omit(d_3[(length(d_3[,1])/2)+1:length(d_3[,1]),]) lows&lt;-na.omit(d_3[1:(length(d_3[,1])/2),]) # Plot ggplot() + geom_bar(data=highs, aes(x = Sex, y=value, fill=col), position=&quot;stack&quot;, stat=&quot;identity&quot;, width = 0.5) + geom_bar(data=lows, aes(x = Sex, y=-value, fill=fct_inorder(col)), position=&quot;stack&quot;, stat=&quot;identity&quot;, width = 0.5) + geom_hline(yintercept = 0, color =c(&quot;white&quot;)) + scale_fill_identity(&quot;Percent&quot;, labels = mylevels, breaks=legend.pal, guide=&quot;legend&quot;) + theme_fivethirtyeight() + coord_flip() + labs(title=mytitle, y=&quot;&quot;,x=&quot;&quot;) + theme(plot.title = element_text(size=14, hjust=0.5)) + theme(axis.text.y = element_text(hjust=0)) + theme(legend.position = &quot;bottom&quot;) 4.5 Theory Likert data is a type of rating scale commonly used in surveys. It’s a bipolar data, representing the attitude to a statement. We can add neutral options to help candidate make choices when they are uncertain. A typical Likert scale may look like Strongly disagree Disagree Agree Strongly agree or Strongly disagree Disagree Neither agree or disagree Agree Strongly agree The options of likert scale can avoid the distortion of the survey result which is likely to be too extreme. 4.6 When to use When the data is from survey and is likert scale. 4.7 External resources 4 ways to visualize Likert Scales: The advantages and disadvantages of different stacked bar chart. Likert Plots in R: Using ggplot to plot diverging stacked bar chart. "],
["stamen-maps-with-ggmap.html", "Chapter 5 Stamen maps with ggmap 5.1 Mutilayerd plots with ggmaps 5.2 Getting Deeper", " Chapter 5 Stamen maps with ggmap Here is an example to get started with ggmap using get_stamenmap() to plot the longitude/latitude maps. The data for the following plots is available at https://simplemaps.com/data/us-cities. The get_stamenmap() function reqiures a bounding box, i.e the top, bottom, left and right latitude/longitude of the map you want to plot. For example, the latitude/longitude for US map are as follows: bbox &lt;- c(bottom = 25.75, top = 49 , right = -67, left = -125) You can find these values from https://www.openstreetmap.org. The other important parameters of this function are zoom and maptype. Higher the zoom level, the more detailed your plot will be. Beaware that ggmap connects to Stamen Map server to download the map, so if your bounding box is large and zoom level is high, it will have to download a lot of data and may take some time. There are differnt types of plots available via Stamen Map like terrain, watercolor, toner which can be set to maptype parameter according to your preference. You can find about avaiable options in help (?get_stamenmap). For the following examples the maptype is set to ‘toner-lite’. Let’s plot the US map. library(ggmap) usmap &lt;- get_stamenmap(bbox = bbox, zoom = 6, maptype = &#39;toner-lite&#39;) ggmap(usmap) Great! We have the US map, now let’s use the US population data to see the population density across nation. Notice that we haven’t included Alaska in the map and hence will be removing the data from Alaska. library(dplyr) df &lt;- read.csv(unz(&#39;resources/ggmap/data/uscities.zip&#39;, &#39;uscities.csv&#39;)) # Removing data of Alaska from dataset df &lt;- df %&gt;% filter(state_name != &#39;Alaska&#39;) # Population density across US using points ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population)) + ggtitle(&#39;Population density across US&#39;) This is not good! Most of the points are overlapping and thus it is not easy to interpret what’s going on here. Let’s try alpha blending. # Population density across US using points ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population), stroke= 0, alpha = 0.1) + ggtitle(&#39;Population density across US&#39;) That’s much better! We can now easily identify the areas where population density is more. You might have noticed there is no light blue dot visible on the plot. This is because it must be lying somewhere between those dense areas. One such location is New York, you can find this out by zooming the plot. We can also look at popluation density using geom_density as follows # Population density across US using Density_2d ggmap(usmap) + geom_density_2d(data = df, mapping = aes(x = lng, y = lat, color = population)) + ggtitle(&#39;Population density across US&#39;) 5.1 Mutilayerd plots with ggmaps We can add multiple layers to the plot as described in earlier chapters. Let’s look at the location of military stations located across US along with population density. # Location of Military units df1 &lt;- df %&gt;% filter(military == TRUE) ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population, text = city), show.legend = F, stroke= 0, alpha = 0.1) + geom_point(data = df1, mapping = aes(x = lng, y = lat , text = city), show.legend = F, color = &#39;red&#39;) + ggtitle(&#39;Military stations across US&#39;) Let’s zoom the map for state of California. # California Boundaries CAbox &lt;- c(bottom = 32.213, top = 42.163 , right = -113.95, left = -124.585) camap &lt;- get_stamenmap(bbox = CAbox, zoom = 6, maptype = &#39;toner-lite&#39;) df3 &lt;- df %&gt;% filter(state_name == &#39;California&#39;) ggmap(camap) + geom_point(data = df3, mapping = aes(x = lng, y = lat, color = population), stroke= 0, alpha = 0.1) + ggtitle(&#39;Population density for California&#39;) 5.2 Getting Deeper This was just a glimpse of what you can do with ggmaps using the get_stamenmap(). Note that Stamen Maps is not limited to US and can be used to plot any part of the world. If you liked this alternative to Google Maps API, I highly recommend you to check the Stamen Maps website http://maps.stamen.com for more details. "],
["shiny.html", "Chapter 6 Shiny 6.1 Part 1 How to Build a Shiny App 6.2 1. Install the shiny package 6.3 2. Template for creating a shiny app 6.4 3. Add elements to user interface using fluidPage() 6.5 4. Build output in server instructions 6.6 5. Share your app 6.7 Part 2 How to Customize Reactions 6.8 1. Reactivity 6.9 3. Summary", " Chapter 6 Shiny Duanyue Yun, Boyu Liu In this tutorial, we will use the cars dataset as an example to wall through the process of building a shiny app. The cars dataset contains various information about a particular car. cars_info &lt;- read.csv(&quot;cars.csv&quot;) 6.1 Part 1 How to Build a Shiny App 6.2 1. Install the shiny package First of all, we can install the shiny package by running the code below. install.packages(&quot;shiny&quot;) 6.3 2. Template for creating a shiny app A shiny app consists of two main components: user interface (ui) and server instructions (server). The user interface will contain the elements that a user sees on your shiny app, which can be input (possible user interactions) and output display. The server instructions will define how the app should react to a user’s action. Therefore, a basic template for creating a shiny app consists of 3 parts as shown below: library(shiny) #1 define user interface ui &lt;- fluidPage() #2 define server instructions server &lt;- function(input, output) {} #3 putting everything together shinyApp(ui = ui, server = server) 6.4 3. Add elements to user interface using fluidPage() The arguments of the fluidPage() function could be Input() functions or Output() functions. 6.4.1 Input functions Inputs define the possible ways a user can interact with our shiny App. For a numerical variable, the input could be a slider that a user can move along to select a certain value. For a categorical variable, the input could be a box where the user can select a particular category from a drop down list. All Input() functions contain 2 required arguments: inputId = and label =. inputId is for us to identify a particular input. Later we can use the same input ID in server instructions to decide the corresponding output. Therefore, to avoid errors, it is better to give a unique name to each input. label is what the user sees on the shiny App, so it should be informative. The common Input() functions supported are: actionButton(), submitButton(), checkboxInput(), checkboxGroupInput(), dateInput(), dateRangeInput(), fileInput(), numericInput(), passwordInput(), radioButtons(), selectInput(), sliderInput(), textInput(). Each Input() function has some specific arguments. For example, the sliderInput() function requires min, max arguments to set the range of the slider and also a value argument which is the default value the user sees when the shiny app is launched. You can find more about the function using ?sliderInput(). For example, we can add a select box by running the code below. ui &lt;- fluidPage( # Add a select box selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)])) 6.4.2 Output functions We can display an output, for example a plot, by adding Output() functions to fluidPage(). Each Output() function requires one argument, which is outputId =. We will talk about how to build output in server instructions. ui &lt;- fluidPage(plotOutput(&quot;histogram&quot;)) The common Output() functions supported are: dataTableOutput(), htmlOutput(), imageOutput(), plotOutput(), tableOutput(), textOutput(), uiOutput(), verbatimTextOutput(). 6.5 4. Build output in server instructions 6.5.1 (1): Save objects you want to display to output$ server &lt;- function(input, output) { output$histogram &lt;- # code } We can use the same name in the form of a string in fluidPage() to display the output. 6.5.2 (2): Build objects with render() The render() functions that are supported include renderDataTable(), renderImage(), renderPlot(), renderPrint(), renderTable(), renderText(), renderUI(). Within render() functions, we could use {} to wrap the code so that we can write multiple lines of code to create more sophisticated output. As an example, the following code builds a histrogram of the variable mpg to our shiny app. Remember to add it to ui() to display it in the shiny app. server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info$mpg, main = &quot;&quot;, xlab = &quot;mpg&quot;) }) } 6.5.3 (3): Use input values with input$ When we use input$, the app will be interactive. For the following example, when the user selects a different variable, the histogram will change accordingly. server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info[[input$varname]], main = &quot;&quot;, xlab = input$varname) }) } 6.6 5. Share your app 6.6.1 Save your app You should save your app to one directory with every file the app needs: app.R (must be the exact file name) datasets, images, css, helper scripts, etc. 6.6.2 Publish your app on Shinyapps.io Go to https://www.shinyapps.io to sign up for an account. When you signed up for a new account on https://www.shinyapps.io, there will be instructions on how to associate your account with your RStudio IDE and how to deploy your app. 6.7 Part 2 How to Customize Reactions 6.8 1. Reactivity 6.8.1 What is reactivity? Let’s think about Microsoft Excel. In Excel, we can type some value into a cell x and type a formula that uses x into a new cell y. Then whenever we change the value in x, y’s value will change correspondingly. This is reactivity, which is also what Shiny does. Shiny has an input object input$x and an output object output$y. Any changes in input$x will cause changes in output$y. So now let’s start with reactive values, which is where reactivity starts in Shiny. 6.8.2 Reactive values Reactive values are what the user selects and depend on Input() functions. In the previous example where we create a select box, the reactive values are the variable that the user selects. Note that reactive values don’t work on their own. They actually work together with reactive functions. 6.8.3 Reactive functions (reactive toolkit) They are a kind of functions that are expected to take reactive values and know what to do with them. They are notified that they need to re-execute whenever the reactive value changes. They are included in the server instructions section to build (and rebuild) an object. We can think of reactivity in R as two-step process. Consider the following example. We use input function selectInput() to get user’s choice. input$varname is the reactive value. When we choose different variables, firstly reactive values will notify the functions which use them that they become outdated. After that its job is over and it’s time for reactive functions to do their jobs, which is rebuild the corresponding object using new values. The process is automatic in shiny. Suppose we want to output the corresponding histogram whenever the user chooses a variable. ui &lt;- fluidPage( # Add a select box selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)]), # Add corresponding output plotOutput(outputId = &quot;histogram&quot;) ) server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info[[input$varname]], main = &quot;&quot;, xlab = input$varname) }) } shinyApp(ui = ui, server = server) 6.8.4 Modularize code with reactive() See the example below. When the user selects a number, the shiny app plots a histogram for that number of N(0,1) variables and also computes the summary statistics. This app has only one reactive value (number the user chooses) but has two objects, a histogram and a block of text which includes the statistics of the data. When the reactive value changes, it will notify these two objects and they will rerun the code to update themself. But the probelm is that because they rerun their code successively, so rnorm(input$num) is called twice. Since rnorm is random, each object generates a different set of values, which means the histogram describes a dataset and the summary of the statistics describes another dataset. # Before ui &lt;- fluidPage( # Add a slider sliderInput(inputId = &quot;num&quot;, label = &quot;Please choose a number.&quot;, min = 1, max = 100, value = 25), # Display the histogram plotOutput(outputId = &quot;hist&quot;), # Display the summary statistics verbatimTextOutput(&quot;stats&quot;) ) server &lt;- function(input, output) { # Build the histogram output$hist &lt;- renderPlot({hist(rnorm(input$num), main = &quot;&quot;, xlab = &quot;num&quot;)}) # Build the object that contains the summary statistics output$stats &lt;- renderPrint({summary(rnorm(input$num))}) } shinyApp(ui = ui, server = server) For example, when we only select one normal variable. It is clear that the histogram and the summary statistics do not correpond to the same data. Can the two objects describe the same data? The answer is yes! The strategy is calling rnorm(input$num) only once and saving the dataset it creates. Then use this dataset downstream when we need it. Shiny provides a function called reactive(), which can wrap a normal expression to create a reactive expression and realize what we hope to achieve. reactive(rnorm(input$num)) In this specific example, we add a code data &lt;- reactive(rnorm(input$num)) to the server and replace rnorm(input$num) in the reactive functions with data(). Note that you should call a reactive expression like a function. So here we use data() instead of data. # After ui &lt;- fluidPage( sliderInput(inputId = &quot;num&quot;, label = &quot;Please choose a number.&quot;, min = 1, max = 100, value = 25), plotOutput(outputId = &quot;hist&quot;), verbatimTextOutput(&quot;stats&quot;) ) server &lt;- function(input, output) { data &lt;- reactive(rnorm(input$num)) output$hist &lt;- renderPlot({hist(data(), main = &quot;&quot;, xlab = &quot;num&quot;)}) output$stats &lt;- renderPrint({summary(data())}) } shinyApp(ui = ui, server = server) Now when will select 1 variable, the two objects will describe the same data. 6.8.5 Prevent reactions with isolate() Sometimes we might want to delay a reactive function. For example, the following shiny app plots a scatterplot between 2 variables of the user’s choose and also allows the user to give the plot a customized title. So there are 3 inputs: the title of the scatterplot, an x variable and a y variable. With our regular code, the title will change instantaneously as the user types. # Before ui &lt;- fluidPage( textInput(inputId = &quot;title&quot;, label = &quot;Enter a title&quot;, value = &quot;displacement vs mpg&quot;), selectInput(&#39;xcol&#39;, &#39;X Variable&#39;, colnames(cars_info)[c(2, 4, 5, 6, 7)]), selectInput(&#39;ycol&#39;, &#39;Y Variable&#39;, colnames(cars_info)[c(2, 4, 5, 6, 7)], selected=colnames(cars_info)[[4]]), plotOutput(&#39;scatterplot&#39;) ) server &lt;- function(input, output) { output$scatterplot &lt;- renderPlot({ plot(cars_info[, c(input$xcol, input$ycol)], main = input$title) }) } shinyApp(ui = ui, server = server) Say we do not want the title to change until the user has chosen two variables. In this case, we can use isolate() to isolate the input title. It returns the result as a non-reactive value. That means the observed object will only react to its changes when other inputs also change. server &lt;- function(input, output) { output$scatterplot &lt;- renderPlot({ plot(cars_info[, c(input$xcol, input$ycol)], # This line isolates the input title main = isolate({input$title})) }) } 6.8.6 Trigger code with observeEvent() we can create an action button or link whose value is initially zero, and increments by one each time it is pressed. When we have an input like action button, we can trigger a response when the user clicks on the button by using observeEvent() function. Examples of an action button include download which allows the user to download a file. actionButton(inputId = &quot;download&quot;, label = &quot;Download&quot;) The observeEvent() function takes two arguments: the first argument is the reactive value(s) it responds to. In our example, it will be the action button. The second armgument is a code block which runs behind the scene whenever the input changes. observeEvent(input$download, {print(input$download)}) Here is how we can use it in our app. Every time we click the Go! button, the observer will update, which is running the block of code print(as.numeric(input$goButton). The result won’t appear in the user panel, but to appear back of our app. ui &lt;- fluidPage( actionButton(inputId = &quot;download&quot;, label = &quot;Download&quot;) ) server &lt;- function(input, output) { observeEvent(input$downloadn, { print(as.numeric(input$download)) }) } shinyApp(ui = ui, server = server) Along with observeEvent() which triggers code, there’s another function called observe(), which does the same thing and it’s a parallel of observeEvent(). But its syntax is more like render*() functions. We just give a block of code to it and it will respond to every reactive value in the code. observe({print(input$download)}) 6.8.7 Delay reactions with eventReactive() Sometimes we don’t want the outputs to change as soon as the user changes some input in the user interface. Instead, we would like to change them when the user clicks an ‘update’ button. In others words, we hope to prevent the output from updating until the user hits the button. The way to do this in Shiny is with the function eventReactive(). It creates a reactive expression that only responds to specific values, similar to reactive() but having different syntax. First we give a reactive value to it. The second argument is the code the function uses to build or rebuild the object when it’s clicked. In addition, similar to observeEvent(), the expression treats this block of code as if it has been isolated with isolate(). data &lt;- eventReactive(input$update, {rnorm(input$num)}) Let’s look at the entire code. ui &lt;- fluidPage( selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)]), actionButton(inputId = &quot;update&quot;, label = &quot;Update&quot;), plotOutput(outputId = &quot;hist&quot;) ) server &lt;- function(input, output) { data &lt;- eventReactive(input$update, {input$varname}) output$hist &lt;- renderPlot({ hist(cars_info[[data()]], main = &quot;&quot;, xlab = data()) }) } shinyApp(ui = ui, server = server) If we choose different variable without clicking Update button, the histogram would not be updated. 6.8.8 Manage state with reactiveValues() We know that the reactive value changes whenever a user changes the input in the user panel. But we cannot set these values in our code. Fortunately, although Shiny doesn’t give us the power to overwrite the input values in our app, it gives us the power to create our own list of reactive values, which you can overwrite. reactiveValues() is a function that creates a list of reactive values to manipulate programmatically. Note that it has nothing to do with input reactive values. rv &lt;- reactiveValues(data = rnorm(100)) Let’s look at an example. If we click mpg vs displacement, the Shiny app would select column mpg and displacement from the cars_info dataset and plot a scatter plot for them. If we click mpg vs weight, it would select column mpg and weight from the cars_info dataset and plot a scatter plot for them. ui &lt;- fluidPage( actionButton(inputId = &quot;scatter1&quot;, label = &quot;mpg vs displacement&quot;), actionButton(inputId = &quot;scatter2&quot;, label = &quot;mpg vs weight&quot;), plotOutput(&quot;scatter&quot;) ) server &lt;- function(input, output) { rv1 &lt;- reactiveValues(data = cars_info[,2], label = &quot;mpg&quot;) rv2 &lt;- reactiveValues(data = cars_info[,4], label = &quot;displacement&quot;) observeEvent(input$scatter1, { rv1$data &lt;- cars_info[,2] rv1$label &lt;- &quot;mpg&quot; rv2$data &lt;- cars_info[,4] rv2$label &lt;- &quot;displacement&quot; }) observeEvent(input$scatter2, { rv1$data &lt;- cars_info[,2] rv1$label &lt;- &quot;mpg&quot; rv2$data &lt;- cars_info[,6] rv2$label &lt;- &quot;weight&quot; }) output$scatter &lt;- renderPlot({ plot(rv1$data, rv2$data, xlab = rv1$label, ylab = rv2$label)}) } shinyApp(ui = ui, server = server) 6.9 3. Summary Till now, We have learnt both syntax and usage of the basic reactive functions in Shiny. Now there are still some important tips we need to provide. We should reduce repetition when we create shiny apps. That is to place code where it will be re-run as little as necessary. Keep in mind that, Code outside the server function will be run once per R session (worker). So you only need it to run once when setting up the R session, outside the server function. For example, codes that load the help file or some library should be placed outside the server function. Code inside the server function will be run once per end user session (connection). Code inside the reactive function will be run once per reaction, which means many times. If you are interested in Shiny and would like to learn more about it, you can go to the official website or download the documentation of Shiny. The relevant resources are listed below. Official website: https://shiny.rstudio.com/ Documantation of pacakge “Shiny”: https://cran.r-project.org/web/packages/shiny/shiny.pdf Share your Shiny apps: https://www.shinyapps.io/ Shiny cheat sheet: https://shiny.rstudio.com/images/shiny-cheatsheet.pdf The main source of this tutorial is the video on the Shiny official website. We adapted it with some new examples based on the cars dataset. Hope this can help you and any suggestion is welcome. "],
["self-reflection-demographical-discoveries-using-tinder-data.html", "Chapter 7 Self-Reflection &amp; Demographical Discoveries Using Tinder Data 7.1 Introduction 7.2 Analysis 7.3 Conclusion 7.4 Final Thoughts", " Chapter 7 Self-Reflection &amp; Demographical Discoveries Using Tinder Data Benjamin Livingston 7.1 Introduction When I told my friends I was doing this, they laughed. After I showed this to my friends, they laughed again. I laughed, too. Our Tinder data is a disturbingly accurate window into our romantic selves. It traces so many of our dating tendencies, from pickiness, to obsession, to desperation, to pushiness. I gained tremendous insight into my romantic habits from this exercise, and I hope you will enjoy it as much as I did. Most importantly, I’ve constructed this in a way that will allow you to easily do this analysis for yourself, too. 7.1.1 For The Taken / Non-Millennial Folk You’re probably going to look at every statistic and graph here and wonder, “what the heck is all this?” Tinder is a dating app that launched in 2012, available from any web browser or smartphone. You create a profile, select your preferred gender, age, and locational proximity for a potential partner, and Tinder provides you a sequence of other users that fit your criteria. Every time a user’s profile pops up, you can either “swipe left” and pass on them, or “swipe right” and like them. If (and only if) you and the other user both swipe right on each other, you are deemed a “match”, and you gain ability to talk to one another. (via Innovation Is Everywhere) User habits vary: some users swipe right on everyone they see, while some users are very picky. There is very little explicit feedback from the app, so the user is forced to form their own conclusions from their personal data, which Tinder allows you to download. 7.1.2 Replicating This Analysis For Yourself I’ve made it possible for you to create all these statistics and graphs for yourself at the click of a button. Your Tinder data can be downloaded at this link. In this GitHub repository, you will find a file called grabyourtinder.R. If you download your Tinder data as instructed, you will receive a zipped file. In that file, there is a JSON labeled “data.JSON”. This is your Tinder data - namely, all your messages and daily statistics. The code I wrote for this project allows any user to extract all of their daily usage statistics from this JSON without the need for additional software. If you extract and copy data.JSON to your R working directory and run the code in grabyourtinder.R, you will be able to create all these graphs and statistics for yourself, and generate a .csv of your Tinder data. Try this!!!! I did all the legwork for you. I’d love to hear what you come up with. If you don’t like what you see, you can throw your laptop out the window and the evidence will disappear forever. A note for non-R users: If you haven’t learned R, this is the perfect time to. It’s free, extremely easy to use, fun to play with, and very powerful. Two recommended free resources if you’d like to try it out: Hadley Wickham’s R For Data Science Roger D. Peng’s R Programming For Data Science 7.1.3 Protecting The Innocent (and Not-So-Innocent) Since my Tinder data JSON file also contains my message data, it will unfortunately not be made available with this project. As you will see soon, there are a lot of messages in there, and thus a plethora of personally identifiable information (for myself and others) that can’t be posted on the internet. Hope you understand. In lieu of this, I have included a .csv file with my daily usage statistics in the GitHub repository, which was extracted from the JSON using my script. 7.1.4 A Fun Twist I will be plotting my Tinder usage over time, and I’m going to add an extra feature to spice it up. This data covers 2014-2015 until the present (we will explain why the start of this range is indefinite later). In Fall 2016, I moved from Pittsburgh to Philadelphia, and then in Summer 2019, I moved from Philadelphia to New York. We will mark those moves in our graphs, and see if we discover any geographic trends as we conduct our analysis. 7.2 Analysis 7.2.1 Our Fun New Tinder Statistics: “Amourmetrics” Opens - the number of times I opened the Tinder app Messages - messages exchanged on the app (split by sent vs. received where stated, combined otherwise) Likes - the number of times I swiped right (a.k.a. “liked” a user) Passes - the number of times I swiped left (a.k.a. “passed” on a user) Swipes - the total number of times I swiped, equal to likes + passes 7.2.2 All-Time Statistics &amp; A Demographical Discovery Let’s start by examining my messaging habits. print(paste0(&#39;Total messages sent: &#39;,sum(bentinder$messages_sent))) ## [1] &quot;Total messages sent: 23047&quot; print(paste0(&#39;Total messages received: &#39;,sum(bentinder$messages_received))) ## [1] &quot;Total messages received: 19156&quot; print(paste0(&#39;Total messages: &#39;,sum(bentinder$messages_sent)+sum(bentinder$messages_received))) ## [1] &quot;Total messages: 42203&quot; I’m a talkative person, so this isn’t particularly surprising. What’s most interesting about this talk-versus-listen trend is how it has varied over time, which we’ll get to in a bit. Of course, your reaction may be a more primal “FORTY TWO THOUSAND MESSAGES?!?!”. If that’s the case, wait until you see my all-time totals across all Tinder statistics. messages = bentinder %&gt;% select(date,messages_sent,messages_received) %&gt;% mutate(message_differential = messages_received - messages_sent) bentinder = bentinder %&gt;% mutate(messages = messages_sent + messages_received) %&gt;% select(-c(messages_sent,messages_received)) bentinder = bentinder %&gt;% mutate(swipes=likes+passes) sapply(bentinder[-1],sum) ## opens likes passes matches messages swipes ## 25081 75404 214505 8777 42203 289909 289,909 swipes! 289,909! This is all mind-blowing… but 289,909?? This could make you laugh, cry, drop your jaw, or just rub your temples and shake your head. But there’s a deeper meaning to this number that I’d like to explore - because considering that I only date men, it’s completely incomprehensible. Think about this for a moment. According to a 2006 study by UCLA’s Gary J. Gates (the most recent readily-available, exhaustive empirical estimate of metropolitan area LGBT populations), the 2005 LGBT populations of the metropolitan areas I’ve lived in were approximately as follows: Pittsburgh: 50,994 Philadelphia: 179,459 New York: 568,903 Furthermore, the LGBT population of Pennsylvania as a whole was 323,454. While these numbers have likely grown in the last decade-and-a-half, they don’t seem to have skyrocketed significantly based on more recent city LGBT population estimates, nor does it make intuitive sense that the number of gay men would have grown astronomically in the last 15 years. Surely, more people are openly LGBT in 2019 than in 2005, but we are making an important distinction between LGBT and openly LGBT (or identifying as LGBT) here. In other words, telling a survey-taker that you are LGBT and being LGBT (and seeking same-sex partners on Tinder) can be two very different things. A quick calculation finds that 273,682 of these swipes happened before I moved to New York. The combined LGBT adult populations of the Pittsburgh and Philadelphia areas is estimated at 230,453, and the entire state only has an estimated 323,454 LGBT adults. If we make a loose assumption that about half of the LGBT residents are male, that would leave about 115,000 gay men in the Philadelphia and Pittsburgh areas and about 162,000 gay men in Pennsylvania at large. I swiped 273,000 times while I lived in Pennsylvania. That means I swiped more than twice the number of available people in my cities and over 1.5 times the number of gay men in my state. Considering I typically set Tinder to only show me people close to my age (almost exclusively within five years), this doesn’t make any sense. This makes me wonder if these LGBT population estimates are even close to accurate. I swiped a lot while out of town (or while using Tinder’s Passport feature) and with visitors from other places, and while I can’t definitively state exactly how much of my swiping was done with people from other metropolitan areas, it probably isn’t enough to explain this trend. Even if only 50,000 of my swipes were done with people residing in my metropolitan area (which would be less than 20% of my overall swipes), these numbers still don’t add up. Tinder typically doesn’t suggest the same person twice, so we can probably rule that out as a major factor. It seems very likely I saw at least 200,000 unique people, and we will make a low-end estimate that 50,000 of them lived in the Philadelphia and Pittsburgh areas. It seems extremely unlikely that that 50,000 of the estimated 115,000 LGBT adult males in those areas are Tinder users close to my age. These numbers suggest there are (and have probably long been) many more gay men in these cities than the aforementioned research stated in 2006, and other self identification-based research has stated since. The linked Gallup article states that “Estimate of LGBT Population Rises to 4.5%”. My data casts serious doubt on the validity of these estimates. In no way am I claiming definitive proof that these figures are wrong, but even a cursory glance at my numbers makes them seem like poor estimates. There are many potential mitigating factors here that prevent any sort of sound empirical proof of this assertion. However, the most parsimonious, plausible explanation is that the true number of gay men in America (or at the very least, in Pennsylvania) hasn’t been anywhere close to properly enumerated in studies that rely on self-identification. Of course… perhaps all this analysis is simply a ploy on my part to deflect from the fact that I have swiped two hundred and eighty-nine thousand times on Tinder. I can’t wrap my head around that number any more than you can. Let’s continue this dive into insanity by examining my all-time daily maximums. sapply(bentinder[-1],max) ## opens likes passes matches messages swipes ## 172 1632 3548 91 509 5144 I don’t need any advanced statistical data analysis to tell you that opening Tinder 172 times in one day and swiping 5144 times in a day is… well I’ll let you pick a word for it. I’m curious though… what was happening on those days? Let’s check the records and find out. bentinder %&gt;% filter(opens==172|likes==1632|passes==3548|matches==91|messages==509|swipes==5144) %&gt;% mutate(day = wday(date,label = T)) ## date opens likes passes matches messages swipes day ## 1 2016-04-10 135 1632 3512 91 289 5144 Sun ## 2 2016-04-12 91 1231 3548 65 241 4779 Tue ## 3 2016-04-13 117 528 1897 72 509 2425 Wed ## 4 2017-02-04 172 1357 3324 81 425 4681 Sat It’s strange: there was nothing remarkable about these days. A quick study of my Google Maps timeline shows that I didn’t go anywhere remotely interesting on any of these days, other than work. I had expected that my record usage would come with travel, but it seems that it just came with boredom. A quick look back at the photos I took on those days confirms the sheer mundanity of my record-setting Tinder marathons. I swiped 12,250 times on the days I took those photos. I will never stop laughing at this. 7.2.3 “It’s Like Batting Average, But For Tinder” Next, we will debut my favorite new statistic from this analysis: the swipe right percentage print(&#39;Swipe right percentage:&#39;) ## [1] &quot;Swipe right percentage:&quot; 100 * (sum(bentinder$likes) / (sum(bentinder$likes) + sum(bentinder$passes))) ## [1] 26.00954 I swipe right on (a.k.a. “like”) only 26% of users. At first, I thought this felt low, and that I was being too picky. Then, I wondered if this might actually be high, since I have no baseline to judge it against. So, let’s answer another simple question: what percentage of users I swiped right on (or “liked”) swiped right on me (“liked” me back)? print(&#39;Match percentage:&#39;) ## [1] &quot;Match percentage:&quot; 100 * sum(bentinder$matches) / sum(bentinder$likes) ## [1] 11.63997 This number is much lower - only 11.6%! I like 26% of users, but only 11.6% of those users like me back. However, it’s important to note that 11.6% of users like me among users that I liked. For the general population, it’s likely a higher percentage, perhaps equal to or greater than 26%. Unfortunately, Tinder does not provide data on how other users swiped you, and we cannot derive this value using probability theory without further information. Still, it’s fascinating to know that of the people I like, only about 1 in 11 like me back. Perhaps I am too picky! For good measure, let’s calculate the percentage of swipes that have yielded a match. print(&#39;Swipes per match:&#39;) ## [1] &quot;Swipes per match:&quot; 100 * sum(bentinder$matches) / sum(bentinder$swipes) ## [1] 3.027502 To be fair, this isn’t quite as bad as I thought. I can deal with swiping 33 times (which takes a minute or two) to get a match. Had this number been 100, I would have felt very differently. We will incorporate these variables into the rest of our analysis as follows. First, we add a swipe right rate, which is equal to the number of times I swipe right divided by my total number of swipes. Second, we add a match rate, a log-adjusted variable that gets higher as more users return my swipes right in kind, and lower as more of the users I liked pass on me. Additional details for math people: To be more specific, we will take the ratio of matches to swipes right, parse any zeros in the numerator or the denominator to 1 (necessary for generating real-valued logarithms), and then take the natural logarithm of this value. This statistic itself won’t be particularly interpretable, but the comparative overall trends will be. 7.2.4 Where &amp; When Did My Swiping Habits Change? We will start our graphing by examining my match rate and swipe right rate over time. bentinder = bentinder %&gt;% mutate(swipe_right_rate = (likes / (likes+passes))) %&gt;% mutate(match_rate = log( ifelse(matches==0,1,matches) / ifelse(likes==0,1,likes))) rates = bentinder %&gt;% select(date,swipe_right_rate,match_rate) match_rate_plot = ggplot(rates) + geom_point(size=0.2,alpha=0.5,aes(date,match_rate)) + geom_smooth(aes(date,match_rate),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=-0.5,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=1) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=-0.5,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=-0.5,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.4) + tinder_theme() + coord_cartesian(ylim = c(-2,-.4)) + ggtitle(&#39;Match Rate Over Time&#39;) + ylab(&#39;&#39;) swipe_rate_plot = ggplot(rates) + geom_point(aes(date,swipe_right_rate),size=0.2,alpha=0.5) + geom_smooth(aes(date,swipe_right_rate),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=.345,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=1) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=.345,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=.345,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.4) + tinder_theme() + coord_cartesian(ylim = c(.2,0.35)) + ggtitle(&#39;Swipe Right Rate Over Time&#39;) + ylab(&#39;&#39;) grid.arrange(match_rate_plot,swipe_rate_plot,nrow=2) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Match rate fluctuates very wildly over time, and there clearly isn’t any sort of annual or monthly trend. It’s cyclical, but not in any obviously traceable manner. My best guess here is that the quality of my profile photos (and perhaps general dating prowess) varied significantly over the last five years, and these peaks and valleys trace the periods when I became more or less attractive to other users. The jumps on the curve are significant, corresponding to users liking me back anywhere from about 20% to 50% of the time. Perhaps this is evidence that the perceived “hot streaks” or “cold streaks” in one’s dating life are a very real thing. Swipe right rate stays much more consistent. There are fewer peaks and valleys, and there’s less overall variation. However, there is a very noticeable dip in Philadelphia. As a native Philadelphian, the implications of this frighten me. We have routinely been derided as having some of the least attractive residents in the nation. I passionately reject that implication. I refuse to accept this as a proud native of the Delaware Valley. That being the case, I’m going to write this off as being a product of disproportionate sample sizes and leave it at that. The uptick in New York is abundantly clear across the board, though. I used Tinder very little in Summer 2019 while preparing for graduate school, which causes many of the usage rate dips we’ll see in 2019 - but there is a huge jump to all-time highs across the board when I move to New York. If you’re an LGBT millennial using Tinder, it’s difficult to beat New York. 7.2.5 A Problem With Dates If you study these tables, you’ll notice the same issue I did - missing data for messages and app opens. bentinder[1:20,-c(8,9)] ## date opens likes passes matches messages swipes ## 1 2014-11-12 0 24 40 1 0 64 ## 2 2014-11-13 0 8 23 0 0 31 ## 3 2014-11-14 0 3 18 0 0 21 ## 4 2014-11-16 0 12 50 1 0 62 ## 5 2014-11-17 0 6 28 1 0 34 ## 6 2014-11-18 0 9 38 1 0 47 ## 7 2014-11-19 0 9 21 0 0 30 ## 8 2014-11-20 0 8 13 0 0 21 ## 9 2014-12-01 0 8 34 0 0 42 ## 10 2014-12-02 0 9 41 0 0 50 ## 11 2014-12-05 0 33 64 1 0 97 ## 12 2014-12-06 0 19 26 1 0 45 ## 13 2014-12-07 0 14 31 0 0 45 ## 14 2014-12-08 0 12 22 0 0 34 ## 15 2014-12-09 0 22 40 0 0 62 ## 16 2014-12-10 0 1 6 0 0 7 ## 17 2014-12-16 0 2 2 0 0 4 ## 18 2014-12-17 0 0 0 1 0 0 ## 19 2014-12-18 0 0 0 2 0 0 ## 20 2014-12-19 0 0 0 1 0 0 print(&#39;----------skipping rows 21 to 169----------&#39;) ## [1] &quot;----------skipping rows 21 to 169----------&quot; bentinder[170:190,-c(8,9)] ## date opens likes passes matches messages swipes ## 170 2015-09-07 5 11 18 1 0 29 ## 171 2015-09-08 4 36 96 3 0 132 ## 172 2015-09-09 8 7 11 2 0 18 ## 173 2015-09-10 2 4 7 0 0 11 ## 174 2015-09-11 3 22 60 3 0 82 ## 175 2015-09-14 2 24 56 0 0 80 ## 176 2015-09-15 1 10 16 1 0 26 ## 177 2015-09-17 0 0 0 1 0 0 ## 178 2015-09-18 2 0 0 0 0 0 ## 179 2015-09-19 1 32 87 2 0 119 ## 180 2015-09-20 1 0 0 1 1 0 ## 181 2015-09-21 2 0 2 1 0 2 ## 182 2015-09-22 1 1 3 0 0 4 ## 183 2015-09-25 1 41 105 3 0 146 ## 184 2015-09-26 0 0 0 2 0 0 ## 185 2015-09-27 0 0 0 1 0 0 ## 186 2015-09-28 0 0 0 1 0 0 ## 187 2015-09-29 9 35 94 3 23 129 ## 188 2015-09-30 11 15 25 3 8 40 ## 189 2015-10-01 2 1 4 0 3 5 ## 190 2015-10-04 0 0 0 0 1 0 bentinder = bentinder %&gt;% select(-c(likes,passes,swipe_right_rate,match_rate)) bentinder = bentinder[-c(1:186),] messages = messages[-c(1:186),] We clearly cannot compile any useful averages or trends using those categories if we’re factoring in data collected before Sep 29, 2015. Therefore, we will restrict our data set to all dates since Sep 29, 2015 moving forward, and all inferences will be made using data from that date on. 7.2.6 Overall Trends Now that we’ve redefined our data set and removed our missing values, let’s examine the relationships between our remaining variables. ggduo(bentinder[2:5], types=list(continuous = wrap(&quot;smooth_loess&quot;, alpha = 0.4,size=0.2))) + tinder_theme() It’s abundantly obvious how much outliers affect this data. Nearly all the points are clustered in the lower left-hand corner of every graph. We can see general long-term trends, but it’s hard to make any sort of deeper inference. There are a lot of very extreme outlier days here, as we can see by studying the boxplots of my usage statistics. tidyben = bentinder %&gt;% gather(key = &#39;var&#39;,value = &#39;value&#39;,-date) ggplot(tidyben,aes(y=value)) + coord_flip() + geom_boxplot() + facet_wrap(~var,scales = &#39;free&#39;,nrow=5) + tinder_theme() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + ggtitle(&#39;Daily Tinder Stats&#39;) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank()) A handful of extreme high-usage dates skew our data, and will make it difficult to view trends in graphs. Thus, henceforth, we will “zoom in” on graphs, displaying a smaller range on the y-axis and hiding outliers in order to better visualize overall trends. 7.2.7 Playing Hard To Get Let’s start zeroing in on trends by “zooming in” on my message differential over time - the daily difference between the number of messages I get and the number of messages I receive. ggplot(messages) + geom_point(aes(date,message_differential),size=0.2,alpha=0.5) + geom_smooth(aes(date,message_differential),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=6,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=0.2) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=6,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=6,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.44) + tinder_theme() + ylab(&#39;Messages Sent/Received In Day&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Message Differential Over Time&#39;) + coord_cartesian(ylim=c(-7,7)) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; The left side of this graph probably doesn’t mean much, since my message differential was closer to zero when I barely used Tinder early on. What’s interesting here is I was talking more than the people I matched with in 2017, but over time that trend eroded. Either I’m talking less, people are talking to me more, or both. Let’s view messages sent and messages received separately and study the trend a little closer. tidy_messages = messages %&gt;% select(-message_differential) %&gt;% gather(key = &#39;key&#39;,value = &#39;value&#39;,-date) ggplot(tidy_messages) + geom_smooth(aes(date,value,color=key),size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=29,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=.3) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=29,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=30,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.2) + tinder_theme() + ylab(&#39;Msg Received &amp; Msg Sent In Day&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Message Rates Over Time&#39;) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; There are a number of possible conclusions you might draw from this graph, and it’s hard to make a definitive statement about it - but my takeaway from this graph was this: I talked way too much in 2017, and over time I learned to send fewer messages and let people come to me. As I did this, the lengths of my conversations eventually reached all-time highs (after the usage dip in Phiadelphia that we’ll discuss in a second). Sure enough, as we’ll see soon, my messages peak in mid-2019 more precipitously than any other usage stat (although we will discuss other potential explanations for this). Learning to push less - colloquially known as playing “hard to get” - appeared to work much better, and now I get more messages than ever and more messages than I send. Again, this graph is open to interpretation. For instance, it’s also possible that my profile simply got better over the last couple years, and other users became more interested in me and started messaging me more. Whatever the case, clearly what I am doing now is working better for me than it was in 2017. 7.2.8 Playing The Game ggplot(tidyben,aes(x=date,y=value)) + geom_point(size=0.5,alpha=0.3) + geom_smooth(color=tinder_pink,se=FALSE) + facet_wrap(~var,scales = &#39;free&#39;) + tinder_theme() + ggtitle(&#39;Daily Tinder Stats Over Time&#39;) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; mat = ggplot(bentinder) + geom_point(aes(x=date,y=matches),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=matches),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=13,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=13,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=13,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,15)) + ylab(&#39;Matches&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Matches Over Time&#39;) mes = ggplot(bentinder) + geom_point(aes(x=date,y=messages),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=messages),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=55,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=55,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=30,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,60)) + ylab(&#39;Messages&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Messages Over Time&#39;) opns = ggplot(bentinder) + geom_point(aes(x=date,y=opens),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=opens),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=32,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=32,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=32,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,35)) + ylab(&#39;App Opens&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Tinder Opens Over Time&#39;) swps = ggplot(bentinder) + geom_point(aes(x=date,y=swipes),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=swipes),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=380,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=380,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=380,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,400)) + ylab(&#39;Swipes&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Swipes Over Time&#39;) grid.arrange(mat,mes,opns,swps) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Even though my swipe right rate went down in Philadelphia, my usage went up (at least at first). This is probably due to Philadelphia having a much larger population than Pittsburgh, but it could also be a product of having a new dating pool after moving. That always causes a flurry of new Tinder activity. The massive dips during the second half of my time in Philadelphia undoubtedly correlates with my preparations for graduate school, which started in early 2018. Then there’s a surge upon arriving in New York and having a month off to swipe, and a significantly larger dating pool. Notice that when I move to New York, all the usage stats peak, but there is an especially precipitous rise in the length of my conversations. Sure, I had more time on my hands (which feeds growth in all these measures), but the relatively large surge in messages suggests I was making more meaningful, conversation-worthy connections than I had in the other cities. This could have something to do with New York, or maybe (as mentioned earlier) an improvement in my messaging style. 7.2.9 “Swipe Night, Part 2” Overall, there is some variation over time with my usage stats, but how much of this is cyclical? We don’t see any evidence of seasonality, but perhaps there’s variation based on the day of the week? Let’s investigate. There isn’t much to see when we compare months (cursory graphing confirmed this), but there’s a clear pattern based on the day of the week. by_day = bentinder %&gt;% group_by(wday(date,label=TRUE)) %&gt;% summarize(messages=mean(messages),matches=mean(matches),opens=mean(opens),swipes=mean(swipes)) colnames(by_day)[1] = &#39;day&#39; mutate(by_day,day = substr(day,1,2)) ## # A tibble: 7 x 5 ## day messages matches opens swipes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Su 39.7 8.43 21.8 256. ## 2 Mo 34.5 6.89 20.6 190. ## 3 Tu 30.3 5.67 17.4 183. ## 4 We 29.0 5.15 16.8 159. ## 5 Th 26.5 5.80 17.2 199. ## 6 Fr 27.7 6.22 16.8 243. ## 7 Sa 45.0 8.90 25.1 344. by_days = by_day %&gt;% gather(key=&#39;var&#39;,value=&#39;value&#39;,-day) ggplot(by_days) + geom_col(aes(x=fct_relevel(day,&#39;Sat&#39;),y=value),fill=tinder_pink,color=&#39;black&#39;) + tinder_theme() + facet_wrap(~var,scales=&#39;free&#39;) + ggtitle(&#39;Tinder Stats By Day of Week&#39;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) rates_by_day = rates %&gt;% group_by(wday(date,label=TRUE)) %&gt;% summarize(swipe_right_rate=mean(swipe_right_rate,na.rm=T),match_rate=mean(match_rate,na.rm=T)) colnames(rates_by_day)[1] = &#39;day&#39; mutate(rates_by_day,day = substr(day,1,2)) ## # A tibble: 7 x 3 ## day swipe_right_rate match_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Su 0.303 -1.16 ## 2 Mo 0.287 -1.12 ## 3 Tu 0.279 -1.18 ## 4 We 0.302 -1.10 ## 5 Th 0.278 -1.19 ## 6 Fr 0.276 -1.26 ## 7 Sa 0.273 -1.40 rates_by_days = rates_by_day %&gt;% gather(key=&#39;var&#39;,value=&#39;value&#39;,-day) ggplot(rates_by_days) + geom_col(aes(x=fct_relevel(day,&#39;Sat&#39;),y=value),fill=tinder_pink,color=&#39;black&#39;) + tinder_theme() + facet_wrap(~var,scales=&#39;free&#39;) + ggtitle(&#39;Tinder Stats By Day of Week&#39;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) Tinder recently labeled Sunday its “Swipe Night”, but for me, that title goes to Saturday. I use the app most then, and the fruits of my labor (matches, messages, and opens that are presumably related to the messages I’m receiving) slowly cascade over the course of the week. I wouldn’t make too much of my match rate dipping on Saturdays. It can take a day or five for a user you liked to open the app, see your profile, and like you back. These graphs suggest that with my increased swiping on Saturdays, my immediate conversion rate goes down, probably for this exact reason. We’ve captured an important feature of Tinder here: it is seldom immediate. Instantaneous responses are rare on Tinder. It’s an app that involves a lot of waiting. You need to wait for a user you liked to like you back, wait for one of you to see the match and send a message, wait for that message to be returned, and so on. This can take a while. It can take days for a match to happen, and then days for a conversation to ramp up. As my Saturday numbers suggest, this often doesn’t happen the same night. So perhaps Tinder is better at looking for a date sometime this week than looking for a date later tonight. 7.2.10 For My Fellow Data Nerds, Or People Who Just Like Graphs Here’s a parallel coordinate plot that allows you to play with the outliers in my usage categories, and see how my luck varied on my high-usage days. Perhaps you’ll notice a trend that I missed. nodates = select(bentinder,-date) parcoords(nodates, rownames = F, brushMode = &quot;1D-axes&quot;, alpha = .4, reorderable = T, queue = T, color = tinder_pink) 7.3 Conclusion 7.3.1 Dubious Demographics The most profound takeaway here is that these numbers cast serious doubt upon many empirical estimates of LGBT populations. We can say with reasonable confidence that my swiping numbers are implausible if estimates of Pennsylvania’s LGBT population are to be believed. 7.3.2 Love Is Bored My Tinder usage peaked when I was doing very little, and it hits its weekly high-water mark on Saturdays. The busier I got in 2019, the more my usage plummeted. Having more time for Tinder clearly leads to more Tinder usage, and having less time for Tinder clearly leads to less Tinder usage. This seems intuitive, but it suggests that romantic obsessions may have as much to do with having nothing to do as they do with actual romance. 7.3.3 Does Location Matter? Well, Maybe. I expected larger differences between localities, but it was difficult to make precise geographic inferences. There were too many other life changes in this period to make any sweeping statements. The drop in swipe right rate upon moving to Philadelphia does stand out (much to my dismay), but certainly not as much as the peaks across the board in New York. Granted, I had a lot of time to swipe when I arrived in New York since I had a month off before school, but it seems that both the quantity and quality of my connections surged. When you’re gay, living in a big city is great for cultural reasons - but we don’t talk enough about how much overall population matters. Clearly, New York was an incredible place to swipe. Location can mean everything when you’re gay. I’ll be curious to see how these New York numbers evolve over time, and if the sheer population of the city allows me to sustain them better than I did in Philadelphia. 7.3.4 The Cinderella Effect My match rate fluctuated very wildly over time, which implies that users’ interest in me varied over time. We can think of this a couple ways - either my representation of myself changed in my profile, or I myself changed and become more or less attractive to other users. Either way, we can interpret this as a sign that we aren’t static, and a person can always get better (or worse) at dating - especially online. 7.3.5 “Playing Hard To Get” May A Be Real Thing The less I dominated my conversations, the longer they got. We can’t prove causality here, but my message differential charts make it appear that a more relaxed, succinct approach to conversation benefitted me. This brings me to my next question… 7.3.6 Can We Solve Dating Using Machine Learning? It would be fascinating to see how my success rates - namely message differential and match rate - are affected by how I use the app. Not using the app isn’t a recipe for dating success, but overdoing it and obsessively swiping and messaging isn’t a good strategy, either. So where’s the middle ground? If we build a model using this data, we may be able to answer these questions, and find an ideal way to use Tinder to maximize success. That’s beyond the scope of this study, but it’s something I’d like to explore down the line. 7.4 Final Thoughts I probably use Tinder too much. I think we’ve established that. Still though, the fruits of my time invested are abundantly evident. I’ve made 8,777 connections, 8,500 of which I probably never would have made otherwise, and some of which became very meaningful to me. We are seldom fortunate enough to have a true quantification of the number of people we’ve interacted with over the years - so as hilariously excessive as all this seems, it’s pretty cool to have such a definitive empirical trace of my 20’s. I’ve learned a lot about myself through this analysis, and I strongly encourage you to run my script and do the same for yourself with your own data. I’d love to hear what you find. Until then, swipe on, my friends. "],
["wordcloud.html", "Chapter 8 Wordcloud 8.1 1. Introduction 8.2 2. Demo of wordcloud2 Package", " Chapter 8 Wordcloud Chengyou Ju and Yujie Wang This Rmd file is created by Chengyou Ju (UNI: cj2624) and Yujie Wang (UNI: yw3442) for STAT GR5702 Community Contribution Group 15. In this file, we will provide a tutorial on how to draw Wordcloud graphs using the wordcloud2 package in R. The dataset in this project are from the demoFreq package. This is the GitHub repo for our project: https://github.com/ju-chengyou/5702_Community_Contribution_15. 8.1 1. Introduction A Wordcloud is a visual representation of text data. Wordclouds are useful for quickly perceiving the most prominent terms, which makes them widely used in media and well understood by the public. A Wordcloud is a collection of words depicted in different sizes. The bigger and bolder the word appears, the greater frequency within a given text and the more important it is. There are two packages in R that can help us draw a wordcloud. wordcloud is the basic package to build the graph, while wordcloud2 package allows more customization. In our demo, we will focus on the wordcloud2 package, which is more widely used. 8.2 2. Demo of wordcloud2 Package For our demo, we will use a built-in dataset demoFreq, which has 1011 observations of 2 variables, words and frequancy. library(wordcloud2) head(demoFreq) ## word freq ## oil oil 85 ## said said 73 ## prices prices 48 ## opec opec 42 ## mln mln 31 ## the the 26 Parameters for wordcloud2 from Rdocumentation data - data frame with word and freqency of the word size - Font size, default is 1. The larger size means the bigger word fontFamily - font used in the word cloud fontWeight - Font weight to use, e.g. normal, bold or 600 color - color of the text, keyword ’random-dark’ and ’random-light’ can be used. backgroundColor - Color of the background minRotation - If the word should rotate, the minimum rotation (in rad) the text should rotate. maxRotation - If the word should rotate, the maximum rotation (in rad) the text should rotate. shuffle - Shuffle the points to draw so the result will be different each time for the same list and settings. rotateRatio - Probability for the word to rotate. Set the number to 1 to always rotate. shape - The shape of the “cloud” to draw. Can be a keyword present. widgetsize - size of the widgets figPath - The path to a figure used as a mask. hoverFunction - Callback to call when the cursor enters or leaves a region occupied by a word. 8.2.1 2.0 Basic Wordcloud Graph Building a wordcloud graph is simple. We can use the wordcloud2 package directly after successfully installing it. wordcloud2(data = demoFreq) As we can see, the word cloud is easy to build and to read. Words with large frequency like ‘said’ and ‘oil’ are displayed in big font size. It is actually an interactive plot. If we hover on a certain word, it will pop up the word with its frequency. 8.2.2 2.1 Font Size We can also modify the font size of the graph. wordcloud2(data = demoFreq, size = 0.5) wordcloud2(data = demoFreq, size = 1.5) 8.2.3 2.2 Color and Background Color The word color can be changed using the “color” argument, while the background color can be changed with “backgroundColor”. wordcloud2(demoFreq, color = &#39;random-dark&#39;) wordcloud2(demoFreq, color = &#39;random-light&#39;) wordcloud2(demoFreq, color = &#39;random-light&#39;, backgroundColor = &quot;black&quot;) wordcloud2(demoFreq, color = rep_len(c(&quot;skyblue&quot;, &quot;blue&quot;), nrow(demoFreq))) 8.2.4 2.3 Shape We can also customize the shape of a wordcloud using the “shape” argument. Here are some examples. wordcloud2(demoFreq, size = 0.5, shape = &#39;star&#39;) wordcloud2(demoFreq, size = 0.5, shape = &#39;pentagon&#39;) 8.2.5 2.4 Rotation We can also do rotation on the wordcloud graph. wordcloud2(demoFreq, minRotation = -pi/6, maxRotation = -pi/6, rotateRatio = 1) 8.2.6 2.5 Language We can draw a wordcloud graph of words in Chinese. wordcloud2(demoFreqC, fontFamily = &quot;????????????&quot;, color = &quot;random-light&quot;, backgroundColor = &quot;black&quot;) 8.2.7 2.6 Customized shape We can build wordcloud with the shape of a word using function letterCloud. letterCloud(demoFreq, word = &quot;R&quot;, color = &quot;random-light&quot;, backgroundColor = &quot;black&quot;) Also, we can create user-defined shape for the wordcloud by simply adding the image we choose to figPath. wordcloud2(demoFreq, figPath =&quot;~/Desktop/batman.png&quot;, size = 1, color = &quot;random-light&quot;,backgroundColor = &quot;black&quot;) batman "],
["workshops.html", "Chapter 9 Workshops 9.1 bookdown", " Chapter 9 Workshops Some groups of students have contributed to the community by running the following workshops. 9.1 bookdown Weixi Yao and Wangzhi Li This introductory workshop on bookdown is designed to give a complete guide to the bookdown package. The workshop is split into two parts. The first part covers the basic information, including what is bookdown, why use bookdown and what are the other options avaiable. The second part serves as a practice session, and each attendee will try to build their own books using the instruction we provide. If the attendees want to know more about bookdown, they can always go back to our slides for reference. We have uplodaed our complete slide to the repo (url: https://github.com/yweixi/EDAV-community-contribution.git; file name: EDAV Community Contribution.pdf). Also, all the materials we use during the workshop can be found in a seperate repo (url: https://github.com/SafeguardLi/SafeguardLi.github.io.git). #R Package Writing Siddhant Shandilya and Mohit Chander Gulla R packages are an ideal way to package and distribute R code and data for re-use by others. This workshop will provide you with an overview of how to create your own pacakge in R. The walkthrough gives step by step instructions on how to define your functions, create a project for your package, embed your functions and its documentation within it and finally how to compile and build it into an R package that is ready to be shared or published. All the materials used in the workshop can be found at: https://github.com/siddhantshandilya/EDAV---Community-Contribution-19 You may refer to the reference links provided at the end of the pdf which goes into further details on how to publish your package on CRAN repository, if you are interested. "],
["cheatsheets.html", "Chapter 10 Cheatsheets 10.1 HTML, JavaScript, and D3 10.2 Leaflet", " Chapter 10 Cheatsheets Some groups of students have contributed to the community by writing the following cheat sheets. 10.1 HTML, JavaScript, and D3 Yitao Liu (yl4343) and Yiyang Sun (ys3284) In addition to the visualization tools we have learned using R, we would like to introduce another powerful visualization tool, D3.js. We created a GitHub Page as well as the GitHub Repository to introduce basic knowledge of building a D3 visualization. We have made cheat sheets for HTML, JavaScript – two cornerstones of coding with D3 library, and a cheat sheet for D3.js. To better help fellow students to kick-off with D3 visualization, we also provided code examples of HTML and D3 in our GitHub Repository. Link to our GitHub Page: https://tonyytliu.github.io/Stat5702_CC60/ Link to our GitHub Repository: https://github.com/tonyytliu/Stat5702_CC60 10.2 Leaflet Di Ye and Qiaoge Zhu This project creates a cheatsheet on leaflet package in R. link: https://drive.google.com/file/d/1NgiAYy7kaoheWbCmJRmu9NxI6Ag7s-VT/view?usp=sharing "],
["videos.html", "Chapter 11 Videos 11.1 CartoDB", " Chapter 11 Videos Some groups of students have contributed to the community by creating video tutorials 11.1 CartoDB Luis Lu and Timothy Huang This tutorial gives a brief overview on getting started with CartoDB, a powerful cloud computing tool that provides geospatial analysis and mapping tools. In this tutorial, we will go over the steps of getting set up on CartoDB, uploading your first dataset, creating your first map visualization, and exploring a few of Carto’s provided geospatial data analysis tools. Link to tutorial video: https://www.youtube.com/watch?v=GxRRXWTMMe8&amp;feature=youtu.be CartoDB "],
["translation.html", "Chapter 12 Translation 12.1 Translation for R &amp; ggplot2 visualization tutorials", " Chapter 12 Translation Some groups of students have contributed to the community by translating useful resources into another language. 12.1 Translation for R &amp; ggplot2 visualization tutorials Yuchen Pei and Jiaqi Tang We translated two online tutorials for visualizaiton in R into Chinese. The first one is called A Comprehensive Guide to Data Visualization in R for Beginners and the second one is called ggplot2: Mastering the basics. Our translation files can be found here: https://github.com/Jasmine1231/EDAV-19Fall-Community_Contribution . "]
]
